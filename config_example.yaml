app:
  name: "人民日报爬虫"
  version: "1.0.0"
  
crawler:
  workers: 5                    # 并发worker数量
  request_interval: 1000ms      # 请求间隔
  timeout: 30s                  # 请求超时
  max_retries: 3               # 最大重试次数
  user_agent: "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
  base_cookies: "xxx"
  base_search_url: "https://data.people.com.cn/rmrb/pd.html"  # 基础搜索URL
  
date_range:
  start_year: 1949
  end_year: 2025
  
storage:
  types: ["csv", "mysql"]      # 启用的存储类型
  csv:
    output_dir: "./data"       # CSV文件输出目录
    file_prefix: "articles"    # 文件名前缀
  mysql:
    host: "localhost"
    port: 3306
    username: "root"
    password: "password"
    database: "people_daily"
    charset: "utf8mb4"
    max_open_conns: 10
    max_idle_conns: 5
    
logging:
  level: "info"                # debug, info, warn, error
  file: "./logs/crawler.log"
  max_size: 100               # MB
  max_backups: 3
  max_age: 28                 # days
